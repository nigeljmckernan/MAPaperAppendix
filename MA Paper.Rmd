---
title: "MA Paper Code Appendix"
author: "Nigel McKernan"
date: "10/18/2020"
output: 
  html_document:
    toc: true
    toc_float: true
    number_sections: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Introduction

This appendix is a companion piece to my Master's Research Project that I completed to fulfill the requirements of my Master's in Economics from Concordia University in Montréal, Québec.

I have 3 main goals that I want to achieve with this document:

* To demonstrate _how_ I executed the analysis of my paper via the programming language R and its vast collection of packages.
* To practice writing [R-Markdown](https://rmarkdown.rstudio.com) documents (what you are reading right now).
  * R-Markdown documents inserts R code into reports, slides, etc., so as to seamlessly transition between regular text documentation, and incorporating the R code written that pertains to the report.
* To demonstrate the following skills to prospective interested parties, such as employers, schools, or other organizations:
  * Research
  * Data analysis, cleaning, and visualization
  * Communicating my research and analysis in a layman-esque manner (as best as I can). 

## Base-R/data.table vs Tidyverse

Apart from the econometrics-focused packages that I'll be using, I prefer using packages and functions from the [Tidyverse](https://tidyverse.org) collection of packages for data manipulation/munging/cleaning, compared to those found in base-R or in the `data.table` package.

That's simply my personal preference with regards to how I best like cleaning and manipulating dirty or untidy data to a point where it is conducive to data analysis, regressions, machine learning, etc.

* I realize `data.table` is faster, though I prefer `dplyr`'s syntax, and the seamless transition into functions from the other Tidyverse packages, like `tidyr`, is unbelievably convenient.
  * If I need the speed that `data.table` is excellent at providing, I'll just use `dtplyr` to speed up my `dplyr` operations.

As an aside, I think people often attribute the `%>%` (pipe) operator from `magrittr` as something that belongs to the Tidyverse. I personally don't see it like that as the pipe works really well with non-Tidyverse packages (inlcuding `data.table`!!). 

With that out of the way, I'll be loading up the `tidyverse` package to load up our essentials.

```{r packages}
library(tidyverse)
library(knitr)
```

  
## Overview of Packages Used

With that previous disclaimer out of the way, the non-Tidyverse packages I'll be using are:

* `plm` -  the main workhorse package for conducting panel-data regression analyses that most closely resembles `lm()`
  * `lme4` & `nlme` won't be used as my model does not really incorporate quadratic regressors (my models fails to reject the RESET test) or other non-linear elements and does not take a mixed-models approach.

* `lmtest` - mainly for  `waldtest()` for comparing models, and `coeftest()` for testing the significance of my regressors while incorporating different estimated covariance matrices from `plm`
  * `sandwich` is not used as the more useful covariance matrix estimators for  panel data are already provided in `plm`
    * i.e. `vcovSCC()`

* `foreach` - the main reason why I like the this package so much is its `for` loop returns an _object_. Not only that, but via the `.combine` argument, you can really customize the output `foreach()` returns.
  * For this project, I want a _single_ data-frame where I iterate through various years, months, and station ID's across Canada from the Environment Canada's website to pull years-worth of climate data at the hourly and daily levels.
    * To do that, the value for my `.combine` argument will be `rbind`, as I want the output from each iteration appended the end of the previous iteration, returning a _single_ data-frame, instead of a nested list that I would have to unnest afterwards.
    * By default, `foreach()` will return a nested list where each element in the list contains the output of the respective iteration.
  * I will not be using a parallelized backend such as `doParallel` as pulling data via http(s) only works sequentially when I've tried it, so the `%do%` or `%doSEQ%` methods will work fine.

* `tempdisagg` - I will not be covering much of the theory of temporal disaggregation, but this package will be used to temporally disaggregate the socioeconomic data from Statistics Canada that is only available annually, down to a monthly frequency, which is the frequency needed to match our dependent variable.
  * I don't attempt to use an indicator series as there is not a clear consensus, or known recommended indicator data, for something like household income, dwelling vacancy rate, or homicide rate
    * As such, I will be using the Denton-Cholette (`method = "denton-cholette"`) method of estimating our AR(1) regressor by regressing on a vector of one's, which does not require an indicator series, and produces very smooth results.
    * As an aside, I _really_ wish there was an attempt to wrap `td()` and `ta()` from `tempdisagg` into a "tidy" way of doing temporal (dis)aggregation
        * I really like what `parsnip` from [Tidymodels](https://tidymodels.org) does to "front-endify" a lot of machine-learning/regression techniques into a coherent and consistent API, and really wish there was an attempt to bring `plm` and `tempdisagg` into that ecosystem.
        
# Exogenous Regressor Data

For my exogenous regressors, they all come from StatsCan, and since the process for how I go about cleaning/pulling/extracting the data for the variable needed is generally the same, I'll only demonstrate this for one variable, for brevity's sake.

The dataset I'll be using is from table 11-10-0135-01 from StatsCan` - "Low income statistics by age, sex and economic family type"

The variable I'll be pulling/cleaning from this table is to extract the percent of persons under the After-Tax-Low-Income-Cutoff, by city, by year.

* In my model, the short-hand for this variable is "LICO"

## Pulling In The Data

Base-R's `read.csv()` function is known to be generally slow, and not the greatest at parsing columns into their respective data types.

`fread()` from `data.table` is blazingly fast, but I generally encounter more errors with it and the columns that get parsed aren't always converted to their appropriate data type.

On the other hand, `read_csv()` or `read_tsv()` from `readr` is very consistent (from my experience) in correctly parsing the data into its appropriate data types, though it is slower than `fread()`.

As such, `readr::read_csv()` will be used the pull the data from our CSV file.

```{r readr}
LICO_Data<- read_csv("Z:\\Documents and Misc Stuff\\MA Paper\\11100135-eng\\11100135.csv")

LICO_Data %>% head() %>% kable()

```


As you can see, some columns do not have names that are clear what they represent in the dataset, or are too long and must be cut down in length for ease of manipulation later.

Let's proceed with that now.

## Removing and Renaming Columns


```{r removing unneeded columns}
LICO_Part_1 <- LICO_Data %>%
  
  select(-DGUID,
           -UOM_ID,
           -SCALAR_ID,
           -VECTOR,
           -COORDINATE,
           -DECIMALS,
           -SYMBOL,
           -STATUS,
           -TERMINATED,
           -SCALAR_FACTOR,
           -UOM)
  
LICO_Part_1 %>% head() %>% knitr::kable()
```

We've eliminated some columns that were either redundant, or had no pertinent information.

However columns like "Low income lines" and "Persons in low income" can be renamed to more succinct names.

```{r renaming}
LICO_Part_2 <- LICO_Part_1 %>%
  
  rename(Persons = `Persons in low income`,
           Lines = `Low income lines`,
           Year = REF_DATE,
           Value = VALUE)

LICO_Part_2 %>% head() %>% knitr::kable()
```

That looks much better.

## Splitting A Column

The "GEO" column is a bit problematic for us as this column contains observations for Canada in aggregate and the various provinces, but _also_ individual cities that have the province name in the observation separated by a comma (,).

* e.g. Winnipeg, Manitoba 
  * (Go Jets Go!)

Instead this column should ideally be split into two:

* One column for "Province/Country"
* The other for "City"

So how do we split this column into two? 

Lucky for us, any time the observation is a city, it's split with a comma (,) from its respective province.

The `separate()` function from `tidyr` makes splitting this easy to do:

```{r splitting}
LICO_Part_3 <- LICO_Part_2 %>%
  
  separate(GEO, c("City","Province"), ", ") 

LICO_Part_3 %>% head() %>% knitr::kable()
```

Since not all observations denote cities, and therefore do not contain a comma, there's no splitting to be done, and so the second column will contain `NA`'s any time that occurs.

This is denoted by the first error box above.

## Filtering Observations

Since this dataset contains superfluous or extra data that is not needed for this analysis, let's filter those observations out.

First, since this project concerns Canadian Metropolitan Areas (CMA's) and _not_ provinces or the country in aggregate, we will be filtering out any observations that are not CMA's.

Additionally, our dependent variable only has data from 2001 to 2019, so we need to filter out years outside of that range to match our dependent variable.

As well, we do not need _all_ the various measures of low income from the "Lines" column. 
We are only concerned with the after-tax variant, based off 1992 as a base year to deflate other years.

Finally, we want to capture *all* persons; not those stratified into different age groups in the "Persons" column.

Those last two requirements can be accomplished by the `stringr` package, which makes dealing with text data very easy.

So, let's filter out the observations in accordance with our needs above:

```{r filtering}
LICO_Part_4 <- LICO_Part_3 %>%
  
  filter(!Province %>% is.na(),
           Year >= 2001 & Year < 2020,
           Persons %>% str_starts("All"),
           Lines %>% str_detect("1992 base"),
           Lines %>% str_detect("after tax"))

LICO_Part_4 %>% head() %>% knitr::kable()
```

Alright, those observations have now been filtered out of our dataset.

Now, we need to _pivot_ this dataset.

## Pivotting

If you're familiar with Pivot Tables in Microsoft Excel or something similar (LibreOffice Calc), then this should be relatively easy to understand.

If not, pivotting data is hard to explain without demonstration, but simply it seeks to transpose individual observations into their *own* columns, or the opposite, to take columns and then _pivot_ them into a single column.

- Our dataset as it is now is in the _former_ situation; we want to turn observations into columns;
  - I want to make certain observations in one column, into *their own* columns:
    - i.e. The "Statistics" column has different types of measurements, with their corresponding value in the "Value" column
      - What I want to achieve is to make a column for every different measure in the "Statistics" column, with their respective value taken from the "Value" column

The end goal of this is to have a unique observation per pairwise combination of city and year, *per row*.


Hard to understand, right? Let's just demonstrate it instead:

First, let's take another look at how our data looks like *before* we pivot it:

```{r pre-pivot}
LICO_Part_4 %>% head() %>% knitr::kable()
```

What I want to achieve is to take those 3 different values in the "Statistics" column, and turn them into their own columns, with their respective value taken from the "Value" column.


```{r pivotting}
LICO_Part_5 <- LICO_Part_4 %>%
  
  select(-Province) %>%
    
#Getting rid of the Province column since I really don't need it anymore.
  
    pivot_wider(names_from = Statistics,
                values_from = Value) %>%

#Renaming the resulting columns to something a little more concise, but based off the renaming, you probably get what they represent.
  
rename (No_Persons = `Number of persons in low income`,
            Percentage = `Percentage of persons in low income`,
            Gap = `Average gap ratio`)

LICO_Part_5 %>% head() %>% knitr::kable()
```


I did some renaming of the resulting columns, but they should be a close enough analogue to what they were named before.


